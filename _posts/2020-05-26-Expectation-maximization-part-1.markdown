---
title: Introduction to Expectation Maximization - part 1 (draft)
layout: post
---

## Motivations

### EM overview


A model with latent variables assumes that sample data are generated by one of many component. 
Typically components are random variables of the same probability distribution, e.g. Poisson or Gaussian, 
each with different parameters. Components are also generated by a random variable which can be discrete, 
as in gaussian mixture model (GMM), or continuous, as in variational autoencoders (VAE).

Expectation Maximisation (EM) is a flexible and general framework to estimate the parameters of 
such models when usual methods for single distribution models donâ€™t work off-the-shelf. Even when EM
can't be applied, it is a useful reference framework, as with variational autoencoders estimation - see part 2.
    
### Why latent variable models?

They give superpowers to the single distribution models we are familiar with by 
accounting for the heterogeneity of the underlying populations when there is no attribute 
that indicates group membership - two examples will follow.


With latent variables it is possible to train generative models i.e. estimate the full distribution
of a data set in an unsupervised way. In my personal experience, the range of applications of generative
models is broader than that of traditional ML models. 

Typically ML models are concerned with the relationship between a target and a set of predictors. They essentially 
perform correlation on steroids, which is great if what you you need is an accurate forecasting 
tool. For any other application, generative models may be better alternatives, for example in
synthetic data generation, forecasting with confidence intervals, or clustering.
          
    
#### References:
This blog article draws on explanations found in the popular [PRML texbook by C. Bishop](https://www.amazon.co.uk/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=redir_mobile_desktop?ie=UTF8&aaxitk=7ttuIh3b5xZ2KXlDWXNKZg&hsa_cr_id=6098124730202&ref_=sbx_be_s_sparkle_asin_1)
and [the Bayesian Methods for Machine Learning](https://www.coursera.org/learn/bayesian-methods-in-machine-learning) MOOC.

## A univariate example
    
Assume the following scenario: Kate manages a restaurant that offers seatings and home delivery. 
She thinks that the kitchen staff may be under-resourced to address sudden spikes in home deliveries between 7-8pm. 
She wishs to compute the probability distribution of 7-8pm delivery orders to inform her resourcing decisions. 
    
Deliveries are managed by a 3rd party aggregator app that provides the number of orders placed over 10 min intervals. 
The chart below describes the number of orders for Mon-Thu over 3 weeks i.e. 12 days shown on the x-axis.

{:refdef: style="text-align: center;"}
![Distribution of orders](/assets/poisson-marginal-likelihood.png){: width="600px"}
{: refdef}

    
We can assume that orders are independent events occurring at a constant rate between 7-8pm. Fridays are 
assumed to be different hence removed from the data set. At first glance she may use a Poisson process to model 
this data however she suspects that the two delivery companies available, Uber Eats and Deliveroo, may have 
different order rates.
    
    
Unfortunately, the aggregator logs do not break down the data by delivery company, which can thus be thought of 
as latent variables. This is an example where going from a single to a mixture of distribution model can 
help model the underling process more accurately.



Expectation Maximization (EM) can be used to estimate models with latent variables. It is an alternative to direct 
optimisation of the maximum likelihood parameters. Maximum likelihood estimation (MLE) is a widely used method 
that asserts that the best estimates are those that explain the evidence - the sample observations - with the highest
probability (or likelihood). 
To understand the strengths of EM, it is useful to start by assuming that components are observed rather than latent to 
estimate the parameters $\theta$ with MLE and then compare with the ME solution. 
    

### Observed components
    
    
If we observe $t_i$, the delivery company, then the likelihood function for the number of orders is
    
    
$$
\prod_{i=1}^N\prod_{t=1}^2 P(x_{i}, t_{i}=c) =
$$

$$
\prod_{i=1}^N\prod_{t=1}^2 P(t_{i}=c)P(x_{i} | t_{i}=c)
$$

Where 

- N is the number of observations
- ${x_i}$ is the number of orders places over a 10-min interval, and
- ${t_i}$ is the delivery company that manages the order i.e. Uber Eats
        (${t_i}=1$) or Deliveroo (${t_i}=2$).

Using the indicator $[t_i=c]$ that is 1 if component 1 generated the observation,
and 0 otherwise, and using ${\pi_c}$ to denote $P(t_{i}=c)$ the likelihood becomes

$$
\prod_{i=1}^N\prod_{c=1}^2 \{\pi_c\frac{\lambda_c^{x_i}}{x_i!}e^{-\lambda_c}\}^{[t_i=c]} 
$$

To be clear our goal is to find $\hat\theta$, the estimate of $\theta$ which maximises the likelihood function.
In this scenario there are 4 parameters, two priors and two Poisson parameters so 
$\hat\theta  = \{\pi1, \pi2, \lambda1, \lambda2\}$.

The next step is to apply a logarithm to simplify the expression. The log-likelihood is 

$$
\begin{equation}
\sum_{i=1}^N\sum_{c=1}^2 {[t_i=c]}\{\log\pi_c+x_i\log\lambda_c-\lambda_c-\log(x_i!)\} =
\tag{1}
\end{equation}
$$

$$
\sum_{c=1}^2{n_c}\log\pi_c+\sum_c^2\sum_i^N\{x_{ic}\log\lambda_c-\lambda_c-\log(x_{ic}!)\}
$$

Where ${n_c}$ is the number of observations assigned to component c and $x_{ic}$ is short for $[t_i=c]*x_i$


The solution for the Poisson parameter, $\lambda_c$, is

$$
\sum_i^N\frac{x_{ic}}{\lambda_c} - {n_c} = 0
$$

$$
\begin{equation}
\lambda_c = \frac{\sum_i^Nx_{ic}}{n_c}
\tag{2}
\end{equation}
$$
    
Which means that the parameter for a component is its average value, which is the same result as the 
single Poisson distribution.

For the component probability $\pi_c$, add the constraint that probabilities must sum to 1 through a Lagrangian
denoted with parameter $\delta$. The log-likelihood is then

$$
\sum_{c=1}^2{n_c}\log\pi_c + \sum_{c=1}^2\sum_i^N\{x_{ic}\log\lambda_c-\lambda_c-\log(x_{ic}!)\} +  \delta(\sum_{c=1}^2\pi_c - 1)
$$

The derivative wrt to $\pi_c$ is 

$$
\frac{n_c}{\pi_c} +  \delta = 0
$$

Multiply by $\pi_c$ on each side, sum over c and remember that the sum of probabilities is 1 to get $\delta = -N$.
Then 

$$
\pi_c = \frac{n_c}{N}
$$

Which means that the prior probability $P({t_i=c})$ is the proportion of observations from component c over all observations N
    

### Latent components with MLE
    
If we can't observe component $t_i$ then things get complicated. 
At first we may think that likelihood estimation is possible because we can write the likelihood function,
$\prod_{i=1}^N P(x_i|\theta)$, by integrating out the hidden variables.

If we can write the objective function, can we optimise it though? Let's see. The likelihood is
    
$$
\prod_i^N\sum_c^2P({x_i},{t_i}) = 
$$

$$
\prod_i^N\sum_c^2P(t_{i}=c)P(x_{i} | t_{i}=c) 
$$

And the log-likelihood is

$$
\begin{equation}
\log P(X|\theta) = \sum_i^N\log\sum_c^2P(t_{i}=c)P(x_{i} | t_{i}=c)
\tag{3}
\end{equation}
$$

As explained in Bishop:


> The presence of the sum prevents the logarithm from acting directly on the
joint distribution, resulting in complicated expressions for the maximum likelihood
solution.


One can ignore this warning and try and solve the above but it is not a straight-forward exercise.

Next we will show that using EM is a better alternative because it is a simple and general
approach that works well for latent variables and distributions of the exponential family.

### Latent components with EM


Imagine that Kate's best friend works at the food delivery aggregator app and can provide some information about 
delivery companies. He's got a model to estimate the probability of the component given the observation $P(t=c|{x_i})$.

In plan English, he has black box that takes a number of orders in a 10 min interval and outputs the probability
that the orders are managed by Uber Eats or Deliveroo. We can use this posterior distribution to get a formula for the likelihood that's almost like the complete-data formula (1)

$$
\begin{equation}
\sum_{i=1}^N\sum_{c=1}^2 P(t_i=c|x_i)\log P(x_i, t_i=c) = 
\tag{4}
\end{equation}
$$

$$
\sum_{i=1}^N\sum_{c=1}^2 P(t_i=c|x_i)\{\log\pi_c+x_i\log\lambda_c-\lambda_c-\log(x_i!)\}
$$

Note how similar the last expression is from (1). The indicator variable is replaced by the posterior, 
i.e. our best guess for the true component that generated ${x_i}$. 

I think of it as a "quasi complete-data" likelihood, 
a state of knowledge that is halfway through the complete data (omniscient) and the incomplete data (ignorant) states.

The log-likelihood function can be solved for $\theta$ because the $\log$ "acts directly on the joint distribution". Let's unpack this.

Solving for $\lambda_c$

$$
\sum_i^N(\frac{P(t_i|x_i)}{\lambda_c}x_i - 1) = 0
$$

$$
\lambda_c = \frac{\sum_i^NP(t_i|x_i)x_i}{\sum_i^NP(t_i|x_i)}
$$

This solution is close to (2). It is the average weighted by the membership assignments.
We don't know which component $x_i$ comes from so we use the best approximation available, $P(t_i|x_i)$

The latent component solution for the prior probability includes the same steps seen before.
Using a Lagrangian and setting the derivative wrt. $\pi_c$ gives

$$
\sum_{i=1}^N \frac{P(t_i=c|x_i)}{\pi_c}+  \delta = 0
$$

And the solution is 

$$
\pi_c = \frac{\sum_i^NP(t_i|x_i)}{N}
$$

The prior probability for a component is the proportion of observations from that 
component, using the posterior probability as an approximation for membership 
assignment.

We have estimated $\hat\theta$, which corresponds to the "M" step of Expectation Maximiization. Kate can use the mixture of Poisson estimates to support her business 
decisions. 

For example, if her restaurant is open for take-away from 7pm she can
estimate the probability of having at least 5 order in the first 20 minutes or, 
using the relationship between Poisson and exponential distributions, she can 
evaluate the probability that the first requests will occur after 7.10pm.

Furthermore, if Kate thought that her friend's black box programme can be further refined she could use $\hat\theta$ to "update" the posterior probabilities using Bayes' rule

$$
P(t_i=c|x_i)=\frac{P(x_i|t_i=c,\theta)P(t_i=c)}{\sum_j^2P(x_i|t_i=j,\theta)P(t_i=j)}
$$

All the ingredients on the RHS of this equation are available. The chart below illustrates
the relationship between mixture parameters and the posterior distribution. Assume that
$(\pi_1, \pi_2, \lambda_1, \lambda_2) = (0.54, 0.46, 1.3, 2.7)$

{:refdef: style="text-align: center;"}
![Distribution of orders](/assets/poissono-estep2.png){: width="700px"}
{: refdef}

Looking at component 1 the posterior probability for $x=1$ is high at 0.7 whereas
5 orders are less likely to come from this component with a probability of 0.04.
With prior probabilities roughly the same, posterior probabilities are close to 
the likelihoood hence the results above make sense. The likelihood of component 1
is higher than component 2 around $x=1$ as its parameter, which is also its
expected value, is close to 1. 

## The EM programme


The imaginary example above suggests why expectation maximization is useful. It's an optimization programme 
used to estimate parameters for probability distributions with latent variables when your best friend can't provide 
reasonably good estimates of the posterior $P(t=c|{x_i})$.


EM addresses the same problem as MLE but takes a different, iterative approach to it. As shown before 
MLE quickly stumbles upon a complex expression whereas in the M step Kate worked around it using the 
"quasi complete-data" likelihood. At this stage three questions are left unanswered:

<ul>
    <li>Where does the "quasi complete-data" function come from?</li>
    <li>How to get a posterior distribution?</li>
    <li>What guarantees of convergence to a maximum does EM provide?</li>
</ul>

The short answers are 
<ul>
<li>It's a lower bound for the likelihood function</li>
<li>Start with random values then iterate through E and M steps</li>
<li>There's no guarantee of global maximum, only points with zero valued gradients</li>
</ul>

The 3rd answer implies that EM can get stuck in local minima or saddle points, which is unsatisfying,
hence in practice EM is run multiple times and only the best estimate is kept.

The long answers require to have the following building bocks in place

<ul>
<li>Introduce the idea of a family of distributions $q$</li>
<li>Use Jensenâ€™s inequality and $q$ to write the lower bound $L$</li>
<li>Find an expression for the difference between $P$ and $L$</li>
</ul>



### Family of distributions $q$
The log-likelihood $\log P(X|\theta)$ from (3), which is hard to solve directly, can be written as

$$
\log P(X|\theta) = \sum_i^N\log\sum_c^Nq(t_i=c)\frac{p(x_i, t_i=c|\theta)}{q(t_i=c)}
$$

because the expression is just multiplied by one. $q$ is a function of the distributions $t_i$ i.e. a set of functions that take discrete values $t_1, â€¦, t_K$ 
and map them to [0, 1] with the constraint that $\sum_c^Kt_c=1$. A particular realisation of $q$ is the posterior
$p(t_i=c|x_i)$ but the expression above is true for any distribution function.

### Jensen's inequality

Jensen's inequality states that for any convex function f and a set of non negative values $p_1, ..., p_K$
that sum to 1, 
then 

$$
f(\sum_c^Kp_cx_i) \geq \sum_c^Kp_cf(x_i)
$$

Replace $f$ by the logarithm and $p_c$ by $q(t=c)$ to get the lower bound $L(\theta, q)$ for the log-likelihood defined above

$$
\log P(X|\theta) = \sum_i^N\log\sum_c^Nq(t_i=c)\frac{p(x_i, t_i=c|\theta)}{q(t_i=c)}
\geq \sum_i^N\sum_c^Nq(t_i=c)\log\frac{p(x_i, t_i=c|\theta)}{q(t_i=c)}
$$

The RHS can be further split out into two terms which will soon become convenient.

$$
L(\theta, q) = \sum_i^N\sum_c^Nq(t_i=c)\log p(x_i, t_i=c|\theta)
- \sum_i^N\sum_c^Nq(t_i=c)\log q(t_i=c)
$$

$L$ is a function of $\theta$, the parameters of the mixture distribution, and $q$, a set of distributions. The LHS part of $L$ is the "quasi complete-data" log-likelihood from (4), because it is the quantity we seek to optimise when $q$ is fixed.

### Kullback Leibler divergence
The difference between $\log P(X|\theta)$ and $L$, its lower bound, is a quantity equal to the Kullback Leilbler divergence between $q$ and the posterior $p(t_i=c|x_i)$. KL divergence is a positive or null value that measures how a distribution $q$ differs from another distribution $p$. Let's prove this result and then use it to understand how EM works.

Start from the KL divergence to get to the expected result:

$$
KL(q(t_i=c)\parallel p(t_i=c|x_i,\theta)) = -\sum_i^N\sum_c^Kq(t_i=c)\log\frac{p(t_i=c|x_i,\theta)}{q(t_i=c)}
$$

Separate out the ratio inside the $\log$ and apply Bayes' theorem to the posterior $p(t_i=c \vert x_i,\theta)$

$$
\sum_i^N\sum_c^Kq(t_i=c)\log q(t_i=c) - \sum_i^N\sum_c^Kq(t_i=c)\log \frac{p(t_i=c, x_i|\theta)}{p(x_i|\theta)}
$$

Further split out the fraction inside the $\log$ on the RHS expression, rearrange the resulting terms and use the fact that $p(x_i \vert \theta)$ does not depend on c and $\sum_c^K q(t_i=c)=1$ as $q$ is a distribution. The KL divergence is equal to

$$
-(\sum_i^N\sum_c^Kq(t_i=c)\log p(t_i=c, x_i|\theta)+\sum_i^N\sum_c^K q(t_i=c)\log q(t_i=c))+\sum_i^Np(x_i|\theta)
$$

So the KL divergence is the sum of the lower bound $L$ (LHS) and the marginalised log-likelihood $\log P(X|\theta)$ (RHS).
Rearranging gives the expected result:

$$
\begin{equation}
\log P(X|\theta) = L(\theta, q(t_i=c)) + KL(q(t_i=c)\parallel p(t_i=c|x_i,\theta))
\tag{5}
\end{equation}
$$

The log-likelihood is the sum of of the lower bound and the KL divergence.

### Putting it all together

The EM framework starts with a random initialisation of either membership assignment or the parameters, then alternates between updating the membership assignment given the parameters, and updating the parameters given the membership assignments.

After each round the log-likelihood increases, and this goes on until there are no longer any significant increase. 

Let's look at each step in detail

#### Step 0

Start with a random guess, $\theta_0$, to kick off the iteratiion.

#### E step

Objective: optimise $L$ wrt. $q$ with $\theta$ fixed at the value from the previous step.


Solution: $q = p(t_i=c \vert x_i, \theta)$.


From (5) $\log P(X \vert \theta)$ does not vary with $q$ so $L$ is maximised when the KL divergence is minimised i.e. $q = p(t_i=c \vert x_i)$ because the difference between a distribution and itself is zero, the minimum value for KL.

If $L$ are red marbles, $q$ are blue marbles and $\log P(X \vert \theta)$ is a jar, the only way to fill it up with red marbles is to remove all the blues.

#### M step

Objective: optimising $L$ wrt. $\theta$ with membership assignments fixed in the previous step i.e. $q = p(t_i=c \vert x_i)$.


Solution: 
$$
{argmax}{_\theta} \sum_{i=1}^N \sum_{c=1}^Kq(t_i=c)\log p(t_i=c, x_i \vert \theta)
$$ 

The actual results depend on the mixture distribution but as with the Poisson example, the presence of the $log$ next to the joint distribution makes the solution straightforward for distributions of the [exponential family](https://en.wikipedia.org/wiki/Exponential_family).

#### Evaluate $\log P(X|\theta)$

Using the estimated parameters from M, evaluate the log-likelihood or the lower bound and stop if there is no significant increase from the previous round of E-M.


Finally, what guarantees do we have that EM will converge? After each iteration $\log P(X|\theta)$ can only be equal or above its previous value because it is greater or equal than the optimised lower bound (M step), which is greater or equal than the non optimised lower bound 
(E step), which is equal to the previous log-likelihood. 

It may help to visualise it
    

{:refdef: style="text-align: center;"}
![Distribution of orders](/assets/donkey-final.png){: width="700px"}
{: refdef}

$ll$ refers to the log-likelihood and $q_*^d$ is $P(t=c \vert x_i)$ with $d$ iteration round. 
Mario can be sure that the EM path can only go up and or be flat. If he chooses this path
he will find Pauline after only 2 rounds. 

The alternative path (MLE) goes straight up, 
which is tempting, however he will have to put up a fight with Kong. Mario is not after the
shorter path, he just wants to find his fiancÃ©, so he'll choose EM.




