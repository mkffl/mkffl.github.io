<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>MathJax example</title>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });

</script>

 	<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async>
	</script>
 <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/js/bootstrap.min.js"></script>
</head>
<body>
<div class="jumbotron">
  <h1>Minimalist RNN - python code perspective</h1>      
  <p>Bootstrap is the most popular HTML, CSS, and JS framework for developing responsive, mobile-first projects on the web.</p>
</div>

<div class="container-fluid">
<div class="row">
<div class="col-sm-2 col-lg-2"></div>
<div class="col-sm-8 col-lg-8">




<h2>Intro</h2>

Recurrent neural networks are a type of neural nets that can model sequences of inputs by memorising the contribution of previous inputs at each time step. Although challenged by recent model architectures like the Transformer which are faster to train, recurrent nets are still widely used as they achieve high accuracy on various tasks like text generation, sentence classification, text-to-speech recognition or time series forecasting.
<br><br>
To understand how neural nets work, I find simplified implementations more useful than looking at the source code from well known packages like Tensorflow or PyTorch. Strip out all the tricks and boiler plate code to focus on the maths behind a model, and suddently complex neural net architectures make sense. 
<br><br>
The maths behind recurrent nets have been covered extensively and a few simplified implementations are available. Probably the most famous was published 4 years ago by A. K. along with an interesting article that explains the reasons behind their good performance. Others have used and commented Karpathy's code, including this excellent article by E. B.
<br><br>
However, even after reading carefully every comment in E. B.'s version of Karpathy's code I still found myself with more questions than answers. The backward pass in lines x-y are especially cryptic. It took me some time and quite a few pages of maths derivation to really understand what was going on.
<br><br>
Here I look at recurrent neural nets from the perspective of a code implementation. For a refresh on the inner workings of recurrent neural net, in particular backpropagation throufh time, I recommend the following web posts. Below, I take E. B.'s fork as the reference code base and focus on the forward and backward pass, found in lossFunc (lines x-y).


<h2>Notations used</h2>

I stay close to the symboles used in the code for easy code-math comparisons:

<ul style="list-style-type:disc;">
  <li>$z^n = W^{xh}\cdot x + W^{hh}\cdot h^{n-1}$</li>
  <li>$h^n = tanh(z^n)$</li>
  <li>$y^n = W^{hy}\cdot h^n)$</li>
  <li>$p^n = softmax(y^n)$</li>
  <li>$Loss^n = crossEntropy(p^n, targets^n)$</li>
</ul>  

$W^{xh}$, $W^{hh}$ and $W^{hy}$ are the parameters we want to optimise with backpropagation. $W^{xh}$ turns the one-hot encoded character vectors into embeddings of size vocab_size. 
$y$ is the vector of unnormalised scores, $p$ refers to the probabilities after normalising $y$ with softmax, $targets$ is the one-hot vector of labels, and $Loss$ is the loss function used for optimisation.


<h2>General comments</h2>

<h4>Input/Outputs</h4>

For reminders, RNNs take a whole sequence as inputs. In this example, sequences are made of characters, of length seq_length and instantiated on lines 245-246:
<pre> <code> 
inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]
targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]
</code></pre> 

The code follows the standard feed-forward / backprop programme for RNNs:
<ol>
  <li>take a sequence of inputs (characters) and their correponding output (next character)</li>
  <li>Run inputs chronologically through the forward and backward passes</li>
  <li>Add up the losses and gradients at each input step, and update gradients after the last input is processed</li>
</ol>

<h4>BPTT</h4>

lossFun takes sequences of fixed size instead of e.g. variable sentence-length sequences.


```python
inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]
targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]
```

That's because the code implements Truncated Backpropagation Through Time. Some sentences can be very long i.e. include many time steps, which would a) require a lot of memory and b) pose computational problems. On a), unrolling the RNN for a very long sequence requires to save the history of all activations. In lossFunc you can see that via the t index for each activation, see for example

```python
# Hidden state value at time step t
 hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)
```

As regards b), this refers to the vanishing gradient problem, a limit of vanilla RNN which more sophisticated memory cells like GRU can solve. This point is well covered here.

<h2>Forward pass</h2>




<h2>Backward pass</h2>

Simple chain rule gives the following partial derivative a time step n:

$$
\frac{\partial Loss^n}{\partial W^{xh}}=\frac{\partial Loss^n}{\partial p^n}\cdot\frac{\partial p^n}{\partial y^n}\cdot\frac{\partial y^n}{\partial h^n}\cdot\frac{\partial h^n}{\partial z^n}\cdot\frac{\partial z^n}{\partial W^{xh}}
$$

It would be tempting to put the pen down and start implementing code but we are not done with the derivation - note that there is both a direct and an indirect dependence between $z^n$ and $W^{xh}$ via $h^{n-1}$:

$$
\frac{\partial z^n}{\partial W^{xh}}=\frac{\partial z^n}{\partial W^{xh}}+\frac{\partial z^n}{\partial h^{n-1}}\cdot\frac{\partial h^{n-1}}{\partial z^{n-1}}\cdot\frac{\partial z^{n-1}}{\partial W^{xh}}
$$

The first term in the above sum is the direct dependence and the second term is the indirect dependence. Next, note that the same dependence applies via previous time steps, for example:

$$
\frac{\partial z^{n-1}}{\partial W^{xh}}=\frac{\partial z^{n-1}}{\partial W^{xh}}+\frac{\partial z^{n-1}}{\partial h^{n-2}}\cdot\frac{\partial h^{n-2}}{\partial z^{n-2}}\cdot\frac{\partial z^{n-2}}{\partial W^{xh}}
$$

And it continues until $h^0$ so the partial derivative becomes
$$
\frac{\partial Loss^n}{\partial W^{xh}}=\frac{\partial Loss^n}{\partial p^n}\cdot\frac{\partial p^n}{\partial y^n}\cdot\frac{\partial y^n}{\partial h^n}\cdot\frac{\partial h^n}{\partial z^n}\sum_{t=1}^{n}\frac{\partial z^t}{\partial h^{t-1}}\cdot\frac{\partial h^{t-1}}{\partial z^{t-1}}\cdot\frac{\partial z^{t-1}}{\partial W^{xh}}
\tag{1}\label{1}
$$

In the above formula, it is obvious that there is a long chain of dependences, which is why vanilla RNN are subject to vanishing gradients, as explained in this article. This is why in practice people use more complex hidden neurons like GRU and LSTM units.
<br><br>
RNN derivations stop here and most articles about Karpathy's RNN code would not explain further, suggesting that the code just implements the above equation. But that's not true and if you look at lines [x-y] you may wonder how the code ties back to the sum above.
<br><br>
Basically, here is what happens in code: as the author loops through each time step in reverse order, he does not compute all the elements from the above sum. Instead, at time step n, he computes the direct dependence but he leaves indirect dependences to be computed at previous steps. Might sound confusing so let's see how it works in detail.
<br><br>
At time step n, let's define the recursive error term e:
$$
e^{nfn}=\frac{\partial Loss^n}{\partial p^n}\cdot\frac{\partial p^n}{\partial y^n}\cdot\frac{\partial y^n}{\partial h^n}
$$
The first index in this term's superscript refers to the indirect dependences back in time, for example step (n-1) is $e^{n-1fn}=\frac{\partial Loss^n}{\partial p^n}\cdot\frac{\partial p^n}{\partial y^n}\cdot\frac{\partial y^n}{\partial h^n}\cdot\frac{\partial h^n}{\partial z^n}\cdot\frac{\partial z^n}{\partial h^{n-1}}$. And since the term is recursive we can write   $e^{n-1fn}=e^{nfn}\cdot\frac{\partial h^n}{\partial z^n}\cdot\frac{\partial z^n}{\partial h^{n-1}}$. For step t this gives 

$$
e^{tfn}=e^{t+1fn}\cdot\frac{\partial h^{t+1}}{\partial z^{t+1}}\cdot\frac{\partial z^{t+1}}{\partial h^{t}}
$$

Using this notation, loss equation (1) becomes
$$
\frac{\partial Loss^n}{\partial W^{xh}}=\sum_{t=1}^{n}e^{tfn}\cdot\frac{\partial h^t}{\partial z^t}\cdot\frac{\partial z^t}{\partial W^{xh}}
$$

In the code the BPTT loop visits each time step n but only computes $e^{nfn}$, the error for the direct dependence; the loop also collects previously computed error terms from variable dh, which allows to update dwxh in the following way:
$$ 
dwxh += \sum_{m=n}^N e^{nfm}\cdot\frac{\partial h^n}{\partial z^n}\cdot\frac{\partial z^n}{\partial W^{xh}}
$$

This is equivalent to computing (3) but it's more efficient from a code point-of-view.

At this stage the backward pass in the code should make sense. Below is a detailed view for Wxh's backpropagation in a simple example with 3 time steps. I explicit the values stored in the code variables at each time step to clarify what they do. Also stripped out the code comments. Lines of interest are:

<pre> <code> 
  dy = np.copy(ps[t])
  dy[targets[t]] -= 1

  dh = np.dot(Why.T, dy) + dhnext

  dhraw = (1 - hs[t] * hs[t]) * dh

  dWxh += np.dot(dhraw, xs[t].T)

  dhnext = np.dot(Whh.T, dhraw)
</code></pre> 

<h4>Time step 3</h4>
<h5><i>(Remember, we are going in reverse time order)</i></h5>

$$ dy \leftarrow \frac{\partial Loss^3}{\partial p^3}\cdot\frac{\partial p^3}{\partial y^3}$$
$$ dh \leftarrow dy\cdot\frac{\partial y^3}{\partial h^3} + \vec{0} = e^{3f3} $$
$$ dhraw \leftarrow e^{3f3}\cdot\frac{\partial h^3}{\partial z^3} $$
$$ dWxh += e^{3f3}\cdot\frac{\partial h^3}{\partial z^3}\cdot\frac{\partial z^3}{\partial W^{xh}} $$
$$ dhnext \leftarrow e^{3f3}\cdot\frac{\partial h^3}{\partial z^3}\cdot\frac{\partial z^3}{\partial h^2} = e^{2f3}$$
 

<h4>Time step 2</h4>

$$ dy \leftarrow \frac{\partial Loss^2}{\partial p^2}\cdot\frac{\partial p^2}{\partial y^2}$$
$$ dh \leftarrow e^{2f2} + e^{2f3}$$
$$ dhraw \leftarrow (e^{2f2} + e^{2f3})\cdot\frac{\partial h^2}{\partial z^2} $$
$$ dWxh += (e^{2f2} + e^{2f3})\cdot\frac{\partial h^2}{\partial z^2}  \cdot\frac{\partial z^2}{\partial W^{xh}} $$
$$ dhnext \leftarrow e^{1f2} + e^{1f3}$$

<h4>Time step 1</h4>

$$ dy \leftarrow \frac{\partial Loss^1}{\partial p^1}\cdot\frac{\partial p^1}{\partial y^1}$$
$$ dh \leftarrow e^{1f1} + e^{1f2} + e^{1f3}$$
$$ dhraw \leftarrow (e^{1f1} + e^{1f2} + e^{1f3})\cdot\frac{\partial h^1}{\partial z^1} $$
$$ dWxh += (e^{1f1} + e^{1f2} + e^{1f3})\cdot\frac{\partial h^1}{\partial z^1}\cdot\frac{\partial z^1}{\partial W^{xh}} $$

By the time we complete t=1, variable dWxh has accumulated all the gradients associated with each recursive error terms. Hence its final value is $\frac{\partial Loss^n}{\partial W^{xh}}$. A simple way to see it is to add up all the lines for dWxh and re-order them to get exactly (1).

</div>
<div class="col-sm-2 col-lg-2"></div>
</div>
</div>




    
</body>
</html>